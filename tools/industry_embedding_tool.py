from __future__ import annotations
from pathlib import Path
import os, sys, json
from typing import List, Dict

# ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).resolve().parents[1]
sys.path.append(str(project_root))

# ì™¸ë¶€ ëª¨ë“ˆ
from langchain.text_splitter import RecursiveCharacterTextSplitter
from config.chroma import get_vector_store
from agents.industry_search import run_search, get_default_config

DOCS_DIR = project_root / "docs"
DOCS_DIR.mkdir(exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VDB ì¡°íšŒ/ì‚½ì… ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _chroma_exists_by_tag(tag: str) -> bool:
    """Chroma ì»¬ë ‰ì…˜ where í•„í„°ë¡œ tagê°€ ìˆëŠ”ì§€ ê°„ë‹¨ ì ê²€."""
    try:
        vdb = get_vector_store()
        col = vdb._collection  # langchain_chroma ë‚´ë¶€ chromadb collection
        res = col.get(where={"kind": "industry"}, limit=1)
        return bool(res and res.get("ids"))
    except Exception:
        return False

def _chroma_insert_texts(texts: List[str]):
    """
    ë©”íƒ€ë°ì´í„°:
      - kind: "industry"
    """
    vdb = get_vector_store()
    metas = [{"kind": "industry"} for _ in texts]
    vdb.add_texts(texts=texts, metadatas=metas)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ê²°ê³¼(JSON) â†’ ì²­í¬ â†’ MD ì €ì¥ â†’ VDB ì ì¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_results(json_path: Path) -> Dict[str, List[Dict]]:
    return json.loads(json_path.read_text(encoding="utf-8"))

def _build_markdown(group: str, items: List[Dict]) -> str:
    lines = [f"# {group}\n"]
    for it in items:
        title = (it.get("title") or "").strip()
        url = (it.get("url") or "").strip()
        content = (it.get("content") or "").strip()
        if title:
            lines.append(f"## {title}")
        if url:
            lines.append(f"- Source: {url}")
        if content:
            lines.append("\n" + content + "\n")
        lines.append("\n---\n")
    return "\n".join(lines)

def _split_markdown(md_text: str, chunk_size=1000, chunk_overlap=150) -> List[str]:
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    return [c.page_content for c in splitter.create_documents([md_text])]

def _persist_group_md(industry: str, group: str, md_text: str) -> Path:
    md_path = DOCS_DIR / f"{industry}_{group}.md"
    md_path.write_text(md_text, encoding="utf-8")
    return md_path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LangGraph ë…¸ë“œ: industry_embedding
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def industry_embedding(
    state: dict,
    *,
    force: bool = False,
    use_global_sources: bool = False,
) -> dict:
    """
    LangGraph add_node(...)ë¡œ ë°”ë¡œ ì—°ê²° ê°€ëŠ¥í•œ ë…¸ë“œ í•¨ìˆ˜.

    - industry / groups / synonyms ëŠ” search ëª¨ë“ˆì˜ í•˜ë“œì½”ë”© ê°’ì„ ì‚¬ìš©
      â†’ get_default_config()ë¡œ ì½ì–´ì˜¨ë‹¤.
    - VDB(Chroma)ì— kind="industry" ê°€ ì´ë¯¸ ìˆìœ¼ë©´ í•´ë‹¹ ê·¸ë£¹ ì„ë² ë”© PASS
    - ì—†ìœ¼ë©´:
        (1) ê²€ìƒ‰ ì‹¤í–‰ â†’ docs/{industry}_search_results.json ì €ì¥
        (2) ê²°ê³¼ë¥¼ MDë¡œ í•©ì¹˜ê³  ì €ì¥
        (3) ì²­í¬ í›„ Chromaì— add_texts(texts, metadatas=...)ë¡œ ì ì¬
          * ë©”íƒ€ë°ì´í„°: kind="industry"
    - ë°˜í™˜: ì…ë ¥ state ê·¸ëŒ€ë¡œ (ë…¸ë“œ ë‚´ë¶€ìš© ì •ë³´ëŠ” stateì— ë„£ì§€ ì•ŠìŒ)
    """
    industry, groups, _syns = get_default_config()
    out_json_path = DOCS_DIR / f"{industry}_search_results.json"

    # 1) ê·¸ë£¹ë³„ë¡œ VDB ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ê²€ìƒ‰ ì „ì— PASS íŒë‹¨)
    to_process = []
    for g in groups:
        if not force and _chroma_exists_by_tag(g):
            print(f"â­ï¸  [SKIP] '{g}' â†’ VDB(kind=industry)ì— ì´ë¯¸ ì¡´ì¬")
            continue
        to_process.append(g)

    if not to_process:
        print("âœ… ëª¨ë“  ê·¸ë£¹ì´ ì´ë¯¸ VDBì— ì¡´ì¬í•©ë‹ˆë‹¤. (ê²€ìƒ‰/ì„ë² ë”© ìƒëµ)")
        return state

    # 2) ê²€ìƒ‰ ì‹¤í–‰ (synonymsì€ search ëª¨ë“ˆ í•˜ë“œì½”ë”© ì‚¬ìš© â†’ override ì „ë‹¬ ì•ˆ í•¨)
    print(f"ğŸ” ê²€ìƒ‰ ì‹¤í–‰: industry={industry}, groups={to_process}, use_global={use_global_sources}")
    out_json_path = run_search(
        industry=industry,
        groups=to_process,
        out_json=out_json_path,
        use_global=use_global_sources,
        synonyms_override=None,
    )

    # 3) ê²°ê³¼ ë¡œë“œ
    results = _load_results(out_json_path)

    # 4) ê·¸ë£¹ë³„ë¡œ MD ì‘ì„± â†’ ì²­í¬ â†’ VDB ì ì¬
    for g in to_process:
        items = results.get(g, [])
        if not items:
            print(f"âš ï¸  [{g}] ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ ìˆì–´ ì„ë² ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        md_text = _build_markdown(g, items)
        md_path = _persist_group_md(industry, g, md_text)
        chunks = _split_markdown(md_text)

        if not chunks:
            print(f"âš ï¸  [{g}] ì²­í¬ ìƒì„± ì‹¤íŒ¨(ë¹ˆ ë¬¸ì„œ). ê±´ë„ˆëœë‹ˆë‹¤.")
            continue

        print(f"ğŸ§© [{g}] ì²­í¬ {len(chunks)}ê°œ â†’ VDB ì ì¬ (kind=industry)")
        _chroma_insert_texts(chunks)

    print("ğŸ‰ ì„ë² ë”© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    return state
