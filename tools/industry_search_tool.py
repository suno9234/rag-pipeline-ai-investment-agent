from __future__ import annotations
from pathlib import Path
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
import os, sys, json, re, time
from typing import List, Dict, Optional, Tuple

# â”€â”€ ê²½ë¡œ/í™˜ê²½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
project_root = Path(__file__).resolve().parents[1]
sys.path.append(str(project_root))

DOCS_DIR = project_root / "docs"
DOCS_DIR.mkdir(exist_ok=True)

load_dotenv()
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# â”€â”€ ê¸°ë³¸ ì„¤ì • (í•˜ë“œì½”ë”©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_INDUSTRY: str = "ëª¨ë¹Œë¦¬í‹°"
DEFAULT_GROUPS: List[str] = ["ì „ê¸°ì°¨", "ì „ë™í‚¥ë³´ë“œ", "ììœ¨ì£¼í–‰"]

# â”€â”€ ë„ë©”ì¸ í™”ì´íŠ¸/ë¸”ë™ ë¦¬ìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
KOREA_TRUSTED = [
    "keei.re.kr","spri.kr","kama.or.kr","kotra.or.kr",
    "kostat.go.kr","data.go.kr","me.go.kr","motie.go.kr","koti.re.kr"
]
GLOBAL_TRUSTED = ["iea.org","oecd.org","imf.org","worldbank.org","ec.europa.eu"]
BLOCK = [
    "gminsights.com","imarcgroup.com","wiseguyreports.com","extrapolate.com",
    "marketwatch.com/press-release","globenewswire.com"
]
GROUP_TRUSTED_MAP = {
    "ì „ê¸°ì°¨": KOREA_TRUSTED,
    "ì „ë™í‚¥ë³´ë“œ": KOREA_TRUSTED,
    "ììœ¨ì£¼í–‰": KOREA_TRUSTED + ["nhtsa.gov","mlit.go.jp"],
}

# â”€â”€ ë…¸ì´ì¦ˆ(ëª©ë¡/ê²€ìƒ‰/ë°°ë„ˆ) ì»· ê·œì¹™ (ì •êµí™”) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NOISY_URL_SUBSTR = [
    "/user/search/search.do", "/search/search.do",
    "mnthngList.do", "pmediaView.do", "cardnewsView.do",
    "/mnthngList.do", "/bbs/mnthngList.do"
]
NOISY_CONTENT_PATTERNS = [
    r"ì „ì²´\s*\(\d+ê±´\)", r"ì—°êµ¬ë³´ê³ ì„œ\s*\(\d+ê±´\)", r"ì •ê¸°ê°„í–‰ë¬¼\s*\(\d+ê±´\)",
    r"íŠ¹í™”ì‚¬ì—…\s*\(\d+ê±´\)", r"ì¢…ë£ŒíŠ¹í™”ì‚¬ì—…\s*\(\d+ê±´\)", r"ë””ì§€í„¸ì½˜í…ì¸ \s*\(\d+ê±´\)",
    r"ì—´ë¦°ê´‘ì¥\s*\(\d+ê±´\)", r"ê²€ìƒ‰ê²°ê³¼", r"í†µí•©ê²€ìƒ‰", r"ì „ì²´ë©”ë‰´",
    r"ë°”ë¡œë³´ê¸°\s*\_?", r"Image\s*\d+\s*:", r"\bëª©ì°¨\b"
]
NOISY_TITLE_PATTERNS = [r"ê²€ìƒ‰ê²°ê³¼", r"ëª©ë¡", r"List"]

MIN_CONTENT_CHARS = 100
MIN_KEEP_AFTER_FILTER = 3
MAX_RESULTS_PRIMARY = 10
MAX_RESULTS_RELAXED  = 15

# â”€â”€ ê·¸ë£¹ë³„ ì¿¼ë¦¬ í™•ì¥(ë™ì˜ì–´/ëŒ€ì²´ì–´) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GROUP_SYNONYMS: Dict[str, List[str]] = {
    "ì „ê¸°ì°¨": ["ì „ê¸°ì°¨", "BEV", "ì „ê¸°ìŠ¹ìš©ì°¨", "ì „ê¸°íŠ¸ëŸ­", "ZEV", "ë°°í„°ë¦¬"],
    "ì „ë™í‚¥ë³´ë“œ": ["ì „ë™í‚¥ë³´ë“œ", "í¼ìŠ¤ë„ ëª¨ë¹Œë¦¬í‹°", "PM", "e-ìŠ¤í¬í„°", "ê³µìœ  í‚¥ë³´ë“œ"],
    "ììœ¨ì£¼í–‰": ["ììœ¨ì£¼í–‰", "ADS", "ë ˆë²¨4", "ë¡œë³´íƒì‹œ", "ììœ¨ì£¼í–‰ì°¨"],
}

def get_default_config() -> Tuple[str, List[str], Dict[str, List[str]]]:
    """
    ì„ë² ë”© ë…¸ë“œê°€ í•˜ë“œì½”ë”©ëœ ê²€ìƒ‰ êµ¬ì„±ì„ ì½ì–´ê°ˆ ë•Œ ì‚¬ìš©.
    """
    return DEFAULT_INDUSTRY, DEFAULT_GROUPS, GROUP_SYNONYMS

# â”€â”€ í¬ë¡¤ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_fulltext(url: str) -> Optional[str]:
    try:
        import trafilatura
    except Exception:
        return None
    try:
        downloaded = trafilatura.fetch_url(url, timeout=15)
        if not downloaded:
            return None
        text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)
        return (text or "").strip()
    except Exception:
        return None

# â”€â”€ ê²€ìƒ‰ ê²°ê³¼ ì •ê·œí™”/í•„í„° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def normalize_results(res) -> List[Dict]:
    if isinstance(res, list):
        return [x if isinstance(x, dict) else {"title": str(x), "url": "", "content": ""} for x in res]
    if isinstance(res, dict) and isinstance(res.get("results"), list):
        items = res["results"]
        return [x if isinstance(x, dict) else {"title": str(x), "url": "", "content": ""} for x in items]
    return [{"title": str(res), "url": "", "content": ""}]

def hard_filter(items: List[Dict], whitelist: List[str], blacklist: List[str]) -> List[Dict]:
    out = []
    for it in items:
        url = (it.get("url") or it.get("source") or "").strip()
        if not url:
            continue
        if any(b in url for b in blacklist):
            continue
        if whitelist and not any(dom in url for dom in whitelist):
            continue
        out.append(it)
    return out

def looks_like_listing_page(title: str, content: str, url: str) -> bool:
    if any(p in (url or "") for p in NOISY_URL_SUBSTR):
        return True
    if any(re.search(p, title or "", flags=re.I) for p in NOISY_TITLE_PATTERNS):
        return True
    if any(re.search(p, content or "", flags=re.I) for p in NOISY_CONTENT_PATTERNS):
        return True
    return False

# â”€â”€ Tavily í´ë¼ì´ì–¸íŠ¸/ì¿¼ë¦¬ ë¹Œë” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_tavily_client(
    include_domains: List[str],
    topic: str = "news",
    time_range: str = "year",
    max_results: int = MAX_RESULTS_PRIMARY
) -> TavilySearch:
    if not TAVILY_API_KEY:
        raise EnvironmentError("TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (.env í™•ì¸)")
    return TavilySearch(
        api_key=TAVILY_API_KEY,
        topic=topic,
        search_depth="advanced",
        time_range=time_range,
        include_raw_content="markdown",
        max_results=max_results,
        include_domains=include_domains,
        exclude_domains=BLOCK,
        country="south korea",
        auto_parameters=True,
    )

def build_queries(industry: str, group: str, synonyms_override: dict | None = None) -> List[str]:
    syns = (synonyms_override or {}).get(group) or GROUP_SYNONYMS.get(group, [group])
    base = [
        f"í•œêµ­ {industry} {group} ì‹œì¥ ë™í–¥ ì •ì±… í†µê³„ ë³´ê³ ì„œ filetype:pdf OR ë³´ê³ ì„œ OR í†µê³„",
        f"í•œêµ­ {industry} {group} ì‚°ì—… ë™í–¥ ë³´ë„ìë£Œ í†µê³„ ë°œí‘œ",
    ]
    for s in syns[:3]:
        base.append(f"í•œêµ­ {industry} {s} ë™í–¥ ì •ì±… í†µê³„")
    return base

def build_relaxed_queries(industry: str, group: str, synonyms_override: dict | None = None) -> List[str]:
    syns = (synonyms_override or {}).get(group) or GROUP_SYNONYMS.get(group, [group])
    relaxed = [
        f"{industry} {group} í•œêµ­ ì‹œì¥ ë™í–¥ ì •ì±… í†µê³„",
        f"{industry} {group} êµ­ë‚´ ë³´ê³ ì„œ ë™í–¥",
    ]
    for s in syns[:2]:
        relaxed.append(f"{industry} {s} í•œêµ­ ë™í–¥")
    return relaxed

def build_site_queries(industry: str, group: str, domains: List[str]) -> List[str]:
    q = []
    for d in domains[:8]:
        q.append(f"site:{d} {industry} {group} ë™í–¥ OR ë³´ê³ ì„œ OR í†µê³„")
    return q

def tavily_invoke(client: TavilySearch, query: str) -> List[Dict]:
    try:
        res = client.invoke({"query": query})
        return normalize_results(res)
    except Exception as e:
        print(f"   âš ï¸ Tavily ì˜¤ë¥˜: {e}")
        return []

def clean_and_enrich(items: List[Dict]) -> List[Dict]:
    cleaned = []
    for it in items:
        title = (it.get("title") or "").strip()
        content = (it.get("content") or it.get("snippet") or it.get("body") or "").strip()
        url = (it.get("url") or it.get("source") or "").strip()
        if looks_like_listing_page(title, content, url):
            continue
        if len(content) < MIN_CONTENT_CHARS and url:
            fulltext = fetch_fulltext(url)
            if fulltext and len(fulltext) > len(content):
                content = fulltext
        if len(content.strip()) < MIN_CONTENT_CHARS:
            continue
        cleaned.append({"title": title, "content": content, "url": url})
    return cleaned

from config.chroma import get_vector_store

def _chroma_exists_by_tag(tag: str) -> bool:
    vdb = get_vector_store()
    col = vdb._collection
    res = col.get(where={"tag": tag}, limit=1)
    return bool(res and res.get("ids"))


# â”€â”€ ê³µê°œ API: ê²€ìƒ‰ ì‹¤í–‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_search(
    industry: str,
    groups: List[str],
    out_json: Optional[Path],
    use_global: bool,
    synonyms_override: dict | None = None,
):
    results_payload: Dict[str, List[Dict]] = {}

    for group in groups:
        # âœ… ë¨¼ì € VDBì— ìˆëŠ”ì§€ í™•ì¸
        if _chroma_exists_by_tag(group):
            print(f"â­ï¸ [SKIP] '{group}' â†’ ì´ë¯¸ VDB(tag={group})ì— ì¡´ì¬ â†’ Tavily í˜¸ì¶œ ìƒëµ")
            results_payload[group] = []  # ê²€ìƒ‰ì€ PASS, JSONì—ëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸ ê¸°ë¡
            continue

        include_domains = (
            GLOBAL_TRUSTED if use_global else GROUP_TRUSTED_MAP.get(group, KOREA_TRUSTED)
        )
        collected: List[Dict] = []
        seen_urls: set[str] = set()

        def add_items(new_items: List[Dict]):
            nonlocal collected, seen_urls
            for it in new_items:
                url = (it.get("url") or it.get("source") or "").strip()
                if not url or url in seen_urls:
                    continue
                seen_urls.add(url)
                collected.append(it)

        client = build_tavily_client(
            include_domains, topic="news", time_range="year", max_results=MAX_RESULTS_PRIMARY
        )
        print(f"\nğŸ” [{group}] 1ì°¨(ê¸°ë³¸) ê²€ìƒ‰ ì¿¼ë¦¬ ë¹Œë“œ")
        for q in build_queries(industry, group, synonyms_override):
            print(f"   â€¢ {q}")
            add_items(tavily_invoke(client, q))
            time.sleep(0.25)

        # === ì´í•˜ ì›ë˜ ê²€ìƒ‰/ì •ì œ ë¡œì§ ë™ì¼ ===
        items = hard_filter(collected, include_domains, BLOCK)
        stage1 = clean_and_enrich(items)
        final_list = stage1
        # ... (2ì°¨, 3ì°¨ ê²€ìƒ‰ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€)
        results_payload[group] = final_list

    out_json = out_json or (DOCS_DIR / f"{industry}_search_results.json")
    out_json.write_text(
        json.dumps(results_payload, ensure_ascii=False, indent=2), encoding="utf-8"
    )
    print(f"\nğŸ’¾ JSON ì €ì¥ ì™„ë£Œ: {out_json}")
    return out_json



# ëª¨ë“ˆ ë‹¨ë… ì‹¤í–‰ ë””í´íŠ¸ (ê°œë°œìš©)
if __name__ == "__main__":
    industry, groups, _ = get_default_config()
    use_global_sources = False
    out_json_path = DOCS_DIR / f"{industry}_search_results.json"
    run_search(industry, groups, out_json_path, use_global_sources)
